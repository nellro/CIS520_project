\subsection{Deep Neural Network}
\label{sec:dnn}

The proposed DNN model consists of 2 hidden layers with 10 neurons 
each, $12$ 
neurons in the input layer (we use 12 features input data 
representation, see Section~\ref{sec:data}) and one node in the 
output layer, see 
Figure~\ref{fig:dnn_graph}. These hyper-parameters were obtained by 
running the set of experiments and comparing the performance, no 
cross-validation was performed.  

Relu activation functions were used, output layer uses logistic 
sigmoid activation function to produce an estimate of the probability 
of VT label.
At the training phase we minimize the cross-entropy loss 
$l_{\log}:\{0,1\}\times[0,1]\rightarrow\mathbb{R}_{+}$:
\begin{equation}
l_{\log}(y,p)= -y\log p - (1-y)\log(1-p)
\end{equation}
We trained the proposed DNN model for 3000 epochs (one forward and 
backward pass of \textit{all} the training examples) with a batch 
size of 128 (number of training examples in one forward/backward 
pass). 

Implementation results are presented in the Table~\ref{tbl:res}.
\paragraph{Implementation.}
We used Keras 2.2.4, the Python Deep Learning library which runs on 
top of TensorFlow. The Python version is 3.6.
We ran experiments using Ubuntu 18.04 LTS with no GPU support.
